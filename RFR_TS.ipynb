{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RFR_TS.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMv3nGvzReanvhZcikIKL9Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahsa26/xgboost_ts/blob/main/RFR_TS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numbers_parser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgoI-QFgabne",
        "outputId": "2b8f88ec-38d6-4114-ab9f-f1c9050a3713"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numbers_parser\n",
            "  Downloading numbers-parser-2.3.12.tar.gz (223 kB)\n",
            "\u001b[K     |████████████████████████████████| 223 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting protobuf>=3.20.1\n",
            "  Downloading protobuf-4.21.2-cp37-abi3-manylinux2014_x86_64.whl (407 kB)\n",
            "\u001b[K     |████████████████████████████████| 407 kB 52.2 MB/s \n",
            "\u001b[?25hCollecting python-snappy\n",
            "  Downloading python_snappy-0.6.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting roman\n",
            "  Downloading roman-3.3-py2.py3-none-any.whl (3.9 kB)\n",
            "Building wheels for collected packages: numbers-parser\n",
            "  Building wheel for numbers-parser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for numbers-parser: filename=numbers_parser-2.3.12-py3-none-any.whl size=243311 sha256=8ebf8a1708c67af108fd3dea3d56da1325d5fd3f96de08e75fe950c5c27c6aa6\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/34/1a/2682ce03fc459520a92573b7bb00b9d24bff0a48e9a6a8f8e3\n",
            "Successfully built numbers-parser\n",
            "Installing collected packages: roman, python-snappy, protobuf, numbers-parser\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.2+zzzcolab20220527125636 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.2 which is incompatible.\n",
            "tensorflow-metadata 1.8.0 requires protobuf<4,>=3.13, but you have protobuf 4.21.2 which is incompatible.\n",
            "googleapis-common-protos 1.56.2 requires protobuf<4.0.0dev,>=3.15.0, but you have protobuf 4.21.2 which is incompatible.\n",
            "google-cloud-bigquery-storage 1.1.2 requires protobuf<4.0.0dev, but you have protobuf 4.21.2 which is incompatible.\n",
            "google-api-core 1.31.6 requires protobuf<4.0.0dev,>=3.12.0; python_version > \"3\", but you have protobuf 4.21.2 which is incompatible.\u001b[0m\n",
            "Successfully installed numbers-parser-2.3.12 protobuf-4.21.2 python-snappy-0.6.1 roman-3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install protobuf==3.20.*\n",
        "from numbers_parser import Document\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import make_scorer, mean_squared_error\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "metadata": {
        "id": "GUo6dtTibJYV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert2matrix(data_arr_on, data_arr_off , look_back, feature_idx0, feature_idx1):\n",
        "    X, Y =[], []\n",
        "    for i in range(len(data_arr_on)-look_back):\n",
        "        d=i+look_back  \n",
        "        X.append(np.concatenate((data_arr_off[i:d,feature_idx0],data_arr_off[i:d,feature_idx1])))\n",
        "        Y.append(data_arr_on[d])\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "# np.concatenate(data_arr_on[i:d,feature_idx0],data_arr_on[i:d,feature_idx1])\n",
        "\n",
        "def model_dnn(optimizer='adam', activation_hl01='relu', activation_hl02='relu', num_node01=32, num_node02=8):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=num_node01, input_dim=look_back*num_features, activation=activation_hl01))\n",
        "    model.add(Dense(num_node02, activation=activation_hl02))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mean_squared_error'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def model_loss(history):\n",
        "    plt.figure(figsize=(16,8))# 16 is x-axis length, 4 is y-axis length\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Test Loss')\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "NCCZeeghb0dX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3G82mzpfZ2Dk",
        "outputId": "d1077b3f-be58-4f75-b6a5-e350a567ca85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "i-0 rows-287\n",
            "i-1 rows-720\n",
            "i-2 rows-744\n",
            "i-3 rows-481\n",
            "i-4 rows-287\n",
            "i-5 rows-720\n",
            "i-6 rows-744\n",
            "i-7 rows-481\n",
            "i-8 rows-287\n",
            "i-9 rows-720\n",
            "i-10 rows-744\n",
            "i-11 rows-481\n",
            "i-12 rows-287\n",
            "i-13 rows-720\n",
            "i-14 rows-744\n",
            "i-15 rows-481\n",
            "16\n",
            "i-0 rows-287\n",
            "i-1 rows-720\n",
            "i-2 rows-744\n",
            "i-3 rows-481\n",
            "i-4 rows-287\n",
            "i-5 rows-720\n",
            "i-6 rows-744\n",
            "i-7 rows-481\n",
            "i-8 rows-287\n",
            "i-9 rows-720\n",
            "i-10 rows-744\n",
            "i-11 rows-481\n",
            "i-12 rows-287\n",
            "i-13 rows-720\n",
            "i-14 rows-744\n",
            "i-15 rows-481\n",
            "(8919,)\n",
            "6243 8919\n",
            "(6243, 7) (2676, 7) (6243,) (2676,)\n",
            "(6241, 4) (6241,) (2674, 4) (2674,)\n",
            "{'max_depth': 4, 'max_features': 'auto', 'n_estimators': 5}\n",
            "Mean Absolute Error: 0.8086128936513755\n",
            "Mean Squared Error: 1.1065597136894463\n",
            "Root Mean Squared Error: 1.051931420620872\n",
            "Pearson Correlation Coefficient (test) 0.064982473589281\n"
          ]
        }
      ],
      "source": [
        "# Read file\n",
        "doc = Document(\"offshore.numbers\")\n",
        "sheets = doc.sheets()\n",
        "#for i in range(len(sheets)):\n",
        "#print(sheets[i].name)\n",
        "print(len(sheets))\n",
        "for i in range(len(sheets)):\n",
        "    tables = sheets[i].tables()#; print('tables', tables)\n",
        "    data_off = np.array(tables[0].rows(values_only=True))\n",
        "    if i%4 == 0:\n",
        "        columns = data_off[0]\n",
        "        data_off = data_off[458:,:]; print('i-{} rows-{}'.format(i, np.shape(data_off)[0]))\n",
        "        if i==0:\n",
        "            all_data_off = np.copy(data_off)\n",
        "        else:\n",
        "            all_data_off = np.concatenate((all_data_off, data_off))\n",
        "    elif i%4 == 3:\n",
        "        data_off = data_off[1:482,:]; print('i-{} rows-{}'.format(i, np.shape(data_off)[0]))\n",
        "        all_data_off = np.concatenate((all_data_off, data_off))\n",
        "    else:\n",
        "        data_off = data_off[1:,:]; print('i-{} rows-{}'.format(i, np.shape(data_off)[0]))\n",
        "        all_data_off = np.concatenate((all_data_off, data_off))\n",
        "\n",
        "#print('all_data (offshore) ', np.shape(all_data_off), all_data_off[:10])\n",
        "\n",
        "label_feature_idx = 2\n",
        "doc = Document(\"nearshore.numbers\")\n",
        "sheets = doc.sheets()\n",
        "#for i in range(len(sheets)):\n",
        "#print(sheets[i].name)\n",
        "print(len(sheets))\n",
        "for i in range(len(sheets)):\n",
        "    tables = sheets[i].tables()#; print('tables', tables)\n",
        "    data_on = np.array(tables[0].rows(values_only=True))\n",
        "    if i%4 == 0:\n",
        "        data_on = data_on[458:,label_feature_idx]; print('i-{} rows-{}'.format(i, np.shape(data_on)[0]))\n",
        "        if i==0:\n",
        "            all_data_on = np.copy(data_on)\n",
        "        else:\n",
        "            all_data_on = np.concatenate((all_data_on, data_on))\n",
        "    elif i%4 == 3:\n",
        "        data_on = data_on[1:482,label_feature_idx]; print('i-{} rows-{}'.format(i, np.shape(data_on)[0]))\n",
        "        all_data_on = np.concatenate((all_data_on, data_on))\n",
        "    else:\n",
        "        data_on = data_on[1:,label_feature_idx]; print('i-{} rows-{}'.format(i, np.shape(data_on)[0]))\n",
        "        all_data_on = np.concatenate((all_data_on, data_on))\n",
        "\n",
        "#print('all_data (nearshore) ', np.shape(all_data_on))\n",
        "\n",
        "time_lag = 9\n",
        "time_lag_on_data = all_data_on[time_lag:]\n",
        "print(np.shape(time_lag_on_data))\n",
        "time_lag_off_data = all_data_off[:len(all_data_off) - time_lag]\n",
        "#print(time_lag_off_data[:10], np.shape(time_lag_off_data))\n",
        "\n",
        "#print(all_data)\n",
        "\n",
        "# approximately 70% data in training set and remaining in test set\n",
        "train_size = int(.7*np.shape(time_lag_off_data)[0]); print(train_size, len(time_lag_off_data))\n",
        "# training data without labels: 2-D\n",
        "train_off, test_off= time_lag_off_data[0:train_size, :], time_lag_off_data[train_size:len(time_lag_off_data)+1, :]\n",
        "# label data: 1-D\n",
        "train_on, test_on = time_lag_on_data[0:train_size], time_lag_on_data[train_size:len(time_lag_on_data)+1]\n",
        "\n",
        "print(train_off.shape, test_off.shape, train_on.shape, test_on.shape)\n",
        "\n",
        "# Prepare time series dataset\n",
        "num_features=2\n",
        "look_back = 2\n",
        "#0:Day, 1:hour, 2:Hs, 3:spr, 4:dir, 5:Tm, 6:Tp\n",
        "feature_idx0 = 2\n",
        "feature_idx1 = 3\n",
        "trainX, trainY = convert2matrix(train_on, train_off, look_back, feature_idx0, feature_idx1)\n",
        "testX, testY = convert2matrix(test_on, train_off, look_back, feature_idx0, feature_idx1)\n",
        "print(np.shape(trainX), np.shape(trainY), np.shape(testX), np.shape(testY))\n",
        "\n",
        "trainX = np.asarray(trainX).astype(np.float32)\n",
        "testX = np.asarray(testX).astype(np.float32)\n",
        "trainY = np.asarray(trainY).astype(np.float32)\n",
        "testY = np.asarray(testY).astype(np.float32)\n",
        "\n",
        "\n",
        "# Number of trees in random forest\n",
        "n_estimators = [5]#, 20, 45, 80]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto']#, 'sqrt', 'log2']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [4]#,5,6,7,8]\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "\n",
        "param_grid = {'n_estimators': n_estimators,\n",
        "                'max_features': max_features,\n",
        "                'max_depth': max_depth\n",
        "                #'min_samples_split': min_samples_split,\n",
        "                #'min_samples_leaf': min_samples_leaf,\n",
        "                #'bootstrap': bootstrap\n",
        "            }\n",
        "\n",
        "rfc = RandomForestRegressor(random_state=42)\n",
        "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
        "CV_rfc.fit(trainX, trainY)\n",
        "print('{}'.format(CV_rfc.best_params_))\n",
        "\n",
        "rfr = RandomForestRegressor(n_estimators=CV_rfc.best_params_['n_estimators'], criterion='squared_error', max_depth=CV_rfc.best_params_['max_depth'], max_features=CV_rfc.best_params_['max_features'])\n",
        "\n",
        "rfr.fit(trainX, trainY)\n",
        "y_pred = rfr.predict(testX)\n",
        "\n",
        "'''\n",
        "rfr = RandomForestRegressor(n_estimators=20, criterion='squared_error', max_depth=num_features)\n",
        "rfr.fit(trainX, trainY)\n",
        "y_pred = rfr.predict(testX)\n",
        "\n",
        "'''\n",
        "print('Mean Absolute Error:', metrics.mean_absolute_error(testY, y_pred))\n",
        "print('Mean Squared Error:', metrics.mean_squared_error(testY, y_pred))\n",
        "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(testY, y_pred)))\n",
        "\n",
        "def cal_cc(testY, test_predict):\n",
        "    part01 = testY - np.mean(testY)\n",
        "    part02 = test_predict - np.mean(test_predict)\n",
        "    num = np.sum(part01 * part02)\n",
        "    den = np.sum(part01**2) * np.sum(part02**2)\n",
        "    if den == 0:\n",
        "        return 0\n",
        "    return num/np.sqrt(den)\n",
        "print(\"Pearson Correlation Coefficient (test)\", cal_cc(testY, y_pred))\n"
      ]
    }
  ]
}