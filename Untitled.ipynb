{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5308ac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numbers-parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45ac01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numbers_parser import Document\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0769d766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2matrix(data_arr_on, data_arr_off , look_back, feature_idx0, feature_idx1):\n",
    "    X, Y =[], []\n",
    "    for i in range(len(data_arr_on)-look_back):\n",
    "        d=i+look_back  \n",
    "        X.append(np.concatenate((data_arr_off[i:d,feature_idx0],data_arr_off[i:d,feature_idx1])))\n",
    "        Y.append(data_arr_on[d])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# np.concatenate(data_arr_on[i:d,feature_idx0],data_arr_on[i:d,feature_idx1])\n",
    "\n",
    "def model_dnn(optimizer='adam', activation_hl01='relu', activation_hl02='relu', num_node01=32, num_node02=8):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=num_node01, input_dim=look_back*num_features, activation=activation_hl01))\n",
    "    model.add(Dense(num_node02, activation=activation_hl02))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mean_squared_error'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def model_loss(history):\n",
    "    plt.figure(figsize=(16,8))# 16 is x-axis length, 4 is y-axis length\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Test Loss')\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "def cal_cc(testY, test_predict):\n",
    "    part01 = testY - np.mean(testY)\n",
    "    part02 = test_predict - np.mean(test_predict)\n",
    "    num = np.sum(part01 * part02)\n",
    "    den = np.sum(part01**2) * np.sum(part02**2)\n",
    "    if den == 0:\n",
    "        return 0\n",
    "    return num/np.sqrt(den)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb8f079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file\n",
    "doc = Document(\"offshore.numbers\")\n",
    "sheets = doc.sheets()\n",
    "#for i in range(len(sheets)):\n",
    "    #print(sheets[i].name)\n",
    "print(len(sheets))\n",
    "for i in range(len(sheets)):\n",
    "    tables = sheets[i].tables()#; print('tables', tables)\n",
    "    data_off = np.array(tables[0].rows(values_only=True))\n",
    "    if i%4 == 0:\n",
    "        columns = data_off[0]\n",
    "        data_off = data_off[458:,:]; print('i-{} rows-{}'.format(i, np.shape(data_off)[0]))\n",
    "        if i==0:\n",
    "            all_data_off = np.copy(data_off)\n",
    "        else:\n",
    "            all_data_off = np.concatenate((all_data_off, data_off))\n",
    "    elif i%4 == 3:\n",
    "        data_off = data_off[1:482,:]; print('i-{} rows-{}'.format(i, np.shape(data_off)[0]))\n",
    "        all_data_off = np.concatenate((all_data_off, data_off))\n",
    "    else:\n",
    "        data_off = data_off[1:,:]; print('i-{} rows-{}'.format(i, np.shape(data_off)[0]))\n",
    "        all_data_off = np.concatenate((all_data_off, data_off))\n",
    "\n",
    "print('all_data (offshore) ', np.shape(all_data_off))\n",
    "\n",
    "label_feature_idx = 2\n",
    "doc = Document(\"nearshore.numbers\")\n",
    "sheets = doc.sheets()\n",
    "#for i in range(len(sheets)):\n",
    "    #print(sheets[i].name)\n",
    "print(len(sheets))\n",
    "for i in range(len(sheets)):\n",
    "    tables = sheets[i].tables()#; print('tables', tables)\n",
    "    data_on = np.array(tables[0].rows(values_only=True))\n",
    "    if i%4 == 0:\n",
    "        data_on = data_on[458:,label_feature_idx]; print('i-{} rows-{}'.format(i, np.shape(data_on)[0]))\n",
    "        if i==0:\n",
    "            all_data_on = np.copy(data_on)\n",
    "        else:\n",
    "            all_data_on = np.concatenate((all_data_on, data_on))\n",
    "    elif i%4 == 3:\n",
    "        data_on = data_on[1:482,label_feature_idx]; print('i-{} rows-{}'.format(i, np.shape(data_on)[0]))\n",
    "        all_data_on = np.concatenate((all_data_on, data_on))\n",
    "    else:\n",
    "        data_on = data_on[1:,label_feature_idx]; print('i-{} rows-{}'.format(i, np.shape(data_on)[0]))\n",
    "        all_data_on = np.concatenate((all_data_on, data_on))\n",
    "\n",
    "print('all_data (nearshore) ', np.shape(all_data_on))\n",
    "\n",
    "\n",
    "#print(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f41cd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# approximately 70% data in training set and remaining in test set\n",
    "train_size = int(.7*np.shape(all_data_off)[0]); print(train_size, len(all_data_off))\n",
    "# training data without labels: 2-D\n",
    "train_off, test_off= all_data_off[0:train_size, :], all_data_off[train_size:len(all_data_off)+1, :]\n",
    "# label data: 1-D\n",
    "train_on, test_on = all_data_on[0:train_size], all_data_on[train_size:len(all_data_on)+1]\n",
    "\n",
    "print(train_off.shape, test_off.shape, train_on.shape, test_on.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a30113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare time series dataset\n",
    "num_features=2\n",
    "look_back = 2\n",
    "#0:Day, 1:hour, 2:Hs, 3:spr, 4:dir, 5:Tm, 6:Tp\n",
    "feature_idx0 = 2\n",
    "feature_idx1 = 3\n",
    "trainX, trainY = convert2matrix(train_on, train_off, look_back, feature_idx0, feature_idx1)\n",
    "testX, testY = convert2matrix(test_on, train_off, look_back, feature_idx0, feature_idx1)\n",
    "print(np.shape(trainX), np.shape(trainY), np.shape(testX), np.shape(testY))\n",
    "\n",
    "trainX = np.asarray(trainX).astype(np.float32)\n",
    "testX = np.asarray(testX).astype(np.float32)\n",
    "trainY = np.asarray(trainY).astype(np.float32)\n",
    "testY = np.asarray(testY).astype(np.float32)\n",
    "\n",
    "xgb = xgboost.XGBRegressor(n_estimators = 10)#tree_method=\"hist\"\n",
    "xgb.fit(trainX, trainY, eval_set=[(testX, testY)])\n",
    "y_pred = xgb.predict(testX)\n",
    "\n",
    "#print('Mean Absolute Error:', metrics.mean_absolute_error(testY, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(testY, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(testY, y_pred)))\n",
    "print(\"Pearson Correlation Coefficient (test)\", cal_cc(testY, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
